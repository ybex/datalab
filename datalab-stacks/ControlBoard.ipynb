{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controlboard for Data Analytics-Stacks\n",
    "## Table of Contents\n",
    "* [Jupyter-Datascience-Notebook](#Jupyter-Datascience-Notebook)    \n",
    "* [Very helpful: Manipulate your Docker environment](#Manipulate-your-Docker-environment)\n",
    "\n",
    "Available stacks to plug into Jupyter:\n",
    "* [Elastic Stack (formerly ELK-Stack)](#Elastic-Stack-(formerly-ELK-Stack))\n",
    "* [PostgreSQL Database, using SQLAlchemy](#PostgreSQL-Database-using-SQLAlchemy)\n",
    "    - [Manage the Stack](#Manage-the-Stack)\n",
    "    - [Connect to Postgres (or any other database!)](#Connect-to-Postgres-(or-any-other-DB))\n",
    "    - [Read data from the DB into a Pandas Dataframe](#Read-data-into-a-Pandas-Dataframe)\n",
    "    - [Create an entity diagram to understand the DB schema](#Create-an-entity-diagram-for-the-database)\n",
    "* [MySQL Database, using SQLAlchemy](#MySQL)\n",
    "* [Neo4j](#Neo4j)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Jupyter Datascience-Notebook\n",
    "#### Summary\n",
    "\n",
    "Your \"standard\" [Jupyter Notebook for Datascience](https://github.com/jupyter/docker-stacks/tree/master/datascience-notebook) plus some additional libraries and the Jupyterlab extensions\n",
    "* [jupyterlab-git: git and GitHub integration](https://github.com/jupyterlab/jupyterlab-git)\n",
    "* [jupyterlab-lsp: Code completion, function definition look-up and more](https://github.com/krassowski/jupyterlab-lsp)\n",
    "* [Code debugger](https://github.com/jupyterlab/debugger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set some variables first - be sure to run this code!\n",
    "Make sure that you edited the file `environment.env.EXAMPLE` to suit your project.\n",
    "* `COMPOSE_PROJECT_NAME`: name of this project. Will show up in all container names associated with this project. No spaces or special characters allowed\n",
    "* `DATALAB_SOURCECODE_DIR`: your Windows directory containing all your source code - including this datalab! Will appear as `/home/jovyan/work` in the Jupyter Notebook\n",
    "* `DATALAB_DATA_DIR`: your Windows directory containing all data. Will be mounted as `/home/jovyan/data` in the Notebook\n",
    "\n",
    "Save the adapted file as a new file `environment.env`. **Only data in either directory will survive the destruction of the Jupyter Notebook container!**\n",
    "\n",
    "Then run this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: COMPOSE_PROJECT_NAME=petproj\n",
      "env: DATALAB_SOURCECODE_DIR=//C/Users/awk02119/Documents/GitHub/petproj\n",
      "env: DATALAB_DATA_DIR=//C/Users/awk02119/Documents/data\n",
      "env: DATALAB_CONTROLBOARD_PORT=12334\n",
      "env: DATALAB_JUPYTER_PORT=8888\n",
      "env: DATALAB_POSTGRES_PORT=5432\n",
      "env: DATALAB_MYSQL_PORT=3306\n",
      "env: DATALAB_ELK_ELASTICSEARCH_PORT1=9200\n",
      "env: DATALAB_ELK_ELASTICSEARCH_PORT2=9300\n",
      "env: DATALAB_ELK_LOGSTASH_PORT1=5000\n",
      "env: DATALAB_ELK_LOGSTASH_PORT2=9600\n",
      "env: DATALAB_ELK_ELASTICSEARCH_KIBANA=5601\n",
      "env: DATALAB_JUPYTER_COMPOSE_PATH=./jupyter/docker-compose.yml\n",
      "env: DATALAB_ELK_PATH=./elk/docker-compose.yml\n",
      "env: DATALAB_POSTGRES_PATH=./postgres/docker-compose.yml\n",
      "env: DATALAB_MYSQL_PATH=./mysql/docker-compose.yml\n",
      "env: DATALAB_NEO4J_PATH=./neo4j/docker-compose.yml\n",
      "env: DATALAB_DOCKER_NETWORK=datalab-network\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Markdown as md\n",
    "import time\n",
    "\n",
    "for line in open('environment.env').readlines():\n",
    "    if not line.strip() or line.strip().startswith('#'):\n",
    "        continue\n",
    "    variable, value = line.strip().split('=', 1)\n",
    "    if variable.endswith('_DIR') or variable.endswith('_PATH'):\n",
    "        # Need to convert Windows paths to the Docker VM running linux\n",
    "        value = value.replace('\\\\', '/')\n",
    "        if ':' in value:\n",
    "            # Absolute path\n",
    "            value = '//' + value.replace(':', '')\n",
    "    %env $variable=$value\n",
    "\n",
    "# Grab port for later\n",
    "jupyter_port = ! echo $DATALAB_JUPYTER_PORT\n",
    "jupyter_port = int(jupyter_port[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start the container\n",
    "Just run this code now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING\u001b[0m: Found orphan containers (petproj_controlboard_1, petproj_postgres_1) for this project. If you removed or renamed this service in your compose file, you can run this command with the --remove-orphans flag to clean it up.\n",
      "Creating petproj_jupyter_1 ... \n",
      "\u001b[1Bting petproj_jupyter_1 ... \u001b[32mdone\u001b[0m\n",
      "Waiting for 5 seconds for the container to spin up\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Your Jupyterlab URL is http://127.0.0.1:8888/?token=47f596326f066fbfc3ddf20701899051cdb52ba6daff8215**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "! sudo -E COMPOSE_FILE=$DATALAB_JUPYTER_COMPOSE_PATH docker-compose up -d\n",
    "\n",
    "# Check the log file for the URL including a unique token. Display the \"correct\" URL with a potentially different port\n",
    "! echo && echo Waiting for 5 seconds for the container to spin up && echo\n",
    "time.sleep(5)\n",
    "log = ! sudo -E COMPOSE_FILE=$DATALAB_JUPYTER_COMPOSE_PATH docker-compose logs jupyter\n",
    "url = 'http://127.0.0.1:8888'\n",
    "for line in log:\n",
    "    if url in line:\n",
    "        break\n",
    "else:\n",
    "    print(log)\n",
    "    raise RuntimeError('Did not find URL in the log above')\n",
    "url = url[:-4] + str(jupyter_port) + line.split(url, 1)[1]\n",
    "md(f\"**Your Jupyterlab URL is {url}**\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopping and cleaning up\n",
    "Stop the Jupyter Notebook container. Container won't be deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping petproj_jupyter_1 ... \n",
      "\u001b[1Bping petproj_jupyter_1 ... \u001b[32mdone\u001b[0m"
     ]
    }
   ],
   "source": [
    "! sudo -E COMPOSE_FILE=$DATALAB_JUPYTER_COMPOSE_PATH docker-compose stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the container. It will be recreated automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING\u001b[0m: Found orphan containers (petproj_controlboard_1, petproj_postgres_1) for this project. If you removed or renamed this service in your compose file, you can run this command with the --remove-orphans flag to clean it up.\n",
      "Removing petproj_jupyter_1 ... \n",
      "\u001b[1BNetwork datalab-network is external, skipping\n"
     ]
    }
   ],
   "source": [
    "! sudo -E COMPOSE_FILE=$DATALAB_JUPYTER_COMPOSE_PATH docker-compose down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to save your entire computational context if you installed additional packages\n",
    "You might change your Docker container by installing new **PIP** Python packages e.g. with `pip install <package name>`. This change will be lost with the container. To quickly save your entire pip environment, including all packages, copy-paste the following into your notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip freeze > /home/jovyan/work/pip-environment.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load your environment again from scratch, e.g. if you re-created your Docker container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r /home/jovyan/work/pip-environment.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you installed additional Python packages with **Anaconda**, `conda install <package name>`, here's how to save the entire conda environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! conda env export -n base > /home/jovyan/work/anaconda-environment.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To re-install all Anaconda packages from this file, do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! conda env update --name base --file /home/jovyan/work/anaconda-environment.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Elastic Stack (formerly ELK-Stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "\n",
    "Elasticsearch, Kibana, Beats, and Logstash. Take data from any source, in any format, then search, analyze, and visualize it in real time.\n",
    "\n",
    "* **Elasticsearch** is a distributed, RESTful search and analytics engine. As the heart of the Elastic Stack, it centrally stores your data for lightning fast search, fine‑tuned relevancy, and powerful analytics that scale with ease.\n",
    "* **Kibana** lets you visualize your Elasticsearch data and navigate the Elastic Stack so you can do anything from tracking query load to understanding the way requests flow through your apps.\n",
    "* **Logstash** is a server-side data processing pipeline that ingests data from a multitude of sources simultaneously, transforms it, and then sends it to your favorite \"stash.\"\n",
    "* **Beats** is the platform for single-purpose data shippers. They send data from hundreds or thousands of machines and systems to Logstash.\n",
    "\n",
    "Note that Beats (e.g. Metricbeat or Systembeat) are not included in this stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connections once the stack has been started\n",
    "* Direct Kibana browser access: [http://localhost:5601](http://localhost:5601)\n",
    "* Elasticsearch access for Windows: [http://localhost:9200](http://localhost:9200)\n",
    "* Access Elasticsearch from **within** a Jupyter container: [http://elasticsearch:9200](http://elasticsearch:9200)\n",
    "* Logstash access from **within** a Jupyter container: [http://logstash:9600](http://elasticsearch:9600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a volume to persist all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker volume create --name=elasticsearch_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start the stack\n",
    "Once pull has completed and containers are running, startup might take 1-2 minutes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo -E COMPOSE_FILE=$DATALAB_ELK_PATH docker-compose up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop and remove the stack (Elasticsearch and Kibana data will be retained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo -E COMPOSE_FILE=$DATALAB_ELK_PATH docker-compose down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete all Elasticsearch and Kibana data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker volume rm elasticsearch_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## PostgreSQL Database using SQLAlchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [PostgreSQL](https://en.wikipedia.org/wiki/PostgreSQL) is a powerful, open source object-relational database system with over 30 years of active development that has earned it a strong reputation for reliability, feature robustness, and performance.\n",
    "* [SQLAlchemy](https://www.sqlalchemy.org/) is a GREAT Python wrapper to talk to almost any database.\n",
    "\n",
    "\n",
    "Check out the [Database getting started Jupyter notebook](database_getting_started.ipynb) for code snippets!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manage the Stack\n",
    "Create a volume to persist all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker volume create postgres_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo -E COMPOSE_FILE=$DATALAB_POSTGRES_PATH docker-compose up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop and remove the stack (database will be retained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo -E COMPOSE_FILE=$DATALAB_POSTGRES_PATH docker-compose down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the actual database and thus all Postgre data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker volume rm postgres_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# MySQL\n",
    "* [MySQL](https://www.mysql.com) is another popular database.\n",
    "* [SQLAlchemy](https://www.sqlalchemy.org/) is a GREAT Python wrapper to talk to almost any database.\n",
    "\n",
    "Check out the [Database getting started Jupyter notebook](database_getting_started.ipynb) for code snippets!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manage the stack\n",
    "Create a volume to persist all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker volume create mysql_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo -E COMPOSE_FILE=$DATALAB_MYSQL_PATH docker-compose up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop and remove the stack (database will be retained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo -E COMPOSE_FILE=$DATALAB_MYSQL_PATH docker-compose down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the actual database and thus all MySQL data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker volume rm mysql_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Neo4j\n",
    "[Neo4j](https://neo4j.com/) is the leading graph database platform. The two plugins [APOC](https://neo4j.com/developer/neo4j-apoc/) and [Graph Data Science](https://neo4j.com/docs/graph-data-science/current/) are included in the stack. All data is saved into a new directory `neo4j` in your `DATALAB_DATA_DIR`.\n",
    "* Neo4j web GUI: http://localhost:7474\n",
    "* Bolt access: http://localhost:7687\n",
    "\n",
    "Neo4j features powerful plugins. You probably want to download [Awesome Procedures APOC](https://github.com/neo4j-contrib/neo4j-apoc-procedures/releases) and/or the [Graph Data Science Library](https://github.com/neo4j/graph-data-science/releases). Simply save the `*.tar` file into the folder `./datalab-stacks/neo4j/plugins` **BEFORE** you start the container."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manage the Stack\n",
    "Start the stack. Note: we assume that you saved the entire datalab in a subfolder `datalab` of your `DATALAB_SOURCECODE_DIR` for plugins to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo -E COMPOSE_FILE=$DATALAB_NEO4J_PATH docker-compose up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop and remove the stack (database will be retained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo -E COMPOSE_FILE=$DATALAB_NEO4J_PATH docker-compose down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Manipulate your Docker environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all running and stopped Docker containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker ps -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all Docker images including their filesizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all volumes (=data volumes if you choose to not mount a Windows directory, for example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker volume ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In desperate need to figure out what's eating up your disk space? This command shows where Docker is using disk space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker system df -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manipulate a container\n",
    "Set a container name (or CONTAINER ID) first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = \"jupyter\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop the container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker stop $container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the running container's logs saved to the Python variable `logoutput`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logoutput = ! sudo docker logs $container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restart an existing (currently stopped) container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker start $container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the container completely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker rm $container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning up and freeing disk space\n",
    "Remove an image (give either it's name or IMAGE ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = \"test\"\n",
    "! sudo docker image rm $image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove all stopped containers at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo docker container prune --force"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove a volume (=data volume, thus potentially deleting your data!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume = \"test\"\n",
    "! sudo docker volume rm $volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Danger zone**: remove all stopped containers, and all images and all volumes that are currently not associated/mounted with a **running container**. Type the following manually:\n",
    "* Delete all stopped containers, all \"dangling\" images, the build cache, any unattached network: ```! sudo docker system prune --force```\n",
    "* To also delete all currently unused images: ```! sudo docker system prune --all --force```\n",
    "* To also delete all currently unused volumes (potentially deleting your data!): ```! sudo docker system prune --volumes --force```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
