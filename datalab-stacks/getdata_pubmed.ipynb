{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bio'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a13e46da0217>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEntrez\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bio'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bio import Entrez\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import Levenshtein as lev\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# query search results from pubmed\n",
    "\n",
    "excellent tutorial: http://biopython.org/DIST/docs/tutorial/Tutorial.html\n",
    "probably PMC will be even more exciting than PubMed as it contains full text articles: https://www.nlm.nih.gov/bsd/difference.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_pubmed(querystring):\n",
    "    \"\"\"\n",
    "    send query to pubmed and get back results in xml\n",
    "    the details of individual publications are retrieved later\n",
    "    \"\"\"\n",
    "    Entrez.email = 'stefanie.tenberg@awk.ch' # should be always stated    \n",
    "    handle = Entrez.esearch(db='pubmed',\n",
    "                            retmode='xml', \n",
    "                            term=querystring,\n",
    "                            usehistory=\"y\")\n",
    "    results = Entrez.read(handle)\n",
    "    handle.close()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 96314 results\n"
     ]
    }
   ],
   "source": [
    "# query pubmed\n",
    "\n",
    "# generate query here (if required): https://www.ncbi.nlm.nih.gov/pubmed/advanced\n",
    "# example: ((Novartis[Affiliation]) AND Biologics) AND Lucentis  ADC OR (Antibody-drug conjugate)\n",
    "querystring=\"covid 19\"\n",
    "search_results=search_pubmed(querystring)\n",
    "count = int(search_results[\"Count\"])\n",
    "print(\"Found %i results\" % count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch all publication details and write them to txt\n",
    "\n",
    "batch_size = 250 #number of publications to fetch at a time\n",
    "t0=time.time()\n",
    "\n",
    "filepath=\"pubmed_data_query_\"+querystring.replace(\" \",\"_\")+\".txt\" #save txt here\n",
    "out_handle = open(filepath, \"w\")\n",
    "for start in range(0, count, batch_size): #loop over all publications found\n",
    "    end = min(count, start + batch_size)\n",
    "    print(\"Going to download record %i to %i of %i\" % (start + 1, end, count))\n",
    "    print(\"time elapsed: %.0d s\" % (time.time()-t0))\n",
    "    fetch_handle = Entrez.efetch(\n",
    "        db=\"pubmed\",\n",
    "        rettype=\"medline\",\n",
    "        retmode=\"text\",\n",
    "        retstart=start,\n",
    "        retmax=batch_size,\n",
    "        webenv=search_results[\"WebEnv\"],\n",
    "        query_key=search_results[\"QueryKey\"],\n",
    "    )\n",
    "    data = fetch_handle.read()\n",
    "    fetch_handle.close()\n",
    "    out_handle.write(data)\n",
    "out_handle.close()\n",
    "print(\"done time elapsed: %.0d s\" % (time.time()-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse txt file to retrieve title, keywords, abstract, author affiliations, ...\n",
    "\n",
    "great parsing intro: https://www.vipinajayakumar.com/parsing-text-with-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'filepath' in locals():\n",
    "    del filepath\n",
    "filepath=\"pubmed_data_query_antibody_drug_conjugate.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read txt and inspect it\n",
    "# with open(filepath) as file:\n",
    "#     file_contents = file.read()\n",
    "#     print(file_contents)\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the regexp we are looking for\n",
    "rx_dict = {\n",
    "    'pub_PMID': re.compile(r'PMID- (?P<pub_PMID>\\d+)\\n'),\n",
    "    'pub_title': re.compile(r'TI  - (?P<pub_title>.*)\\n'),\n",
    "    'pub_keywords': re.compile(r'OT  - (?P<pub_keywords>.*)\\n'),\n",
    "    'pub_abstract': re.compile(r'AB  - (?P<pub_abstract>.*)\\n'),\n",
    "    'pub_affiliation': re.compile(r'AD  - (?P<pub_affiliation>.*)\\n'),\n",
    "    'line_break': re.compile(r'      (?P<line_break>.*)\\n'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_line(line):\n",
    "    \"\"\"\n",
    "    Do a regex search against all defined regexes and\n",
    "    return the key and match result of the first matching regex\n",
    "    \n",
    "    https://www.vipinajayakumar.com/parsing-text-with-python/\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    for key, rx in rx_dict.items():\n",
    "        match = rx.search(line)\n",
    "        if match:\n",
    "            return key, match\n",
    "            \n",
    "    # if there are no matches\n",
    "    return None, None\n",
    "\n",
    "\n",
    "\n",
    "def parse_file(filepath):\n",
    "    \"\"\"\n",
    "    Parse text at given filepath\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : str\n",
    "        Filepath for file_object to be parsed\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : pd.DataFrame\n",
    "        Parsed data\n",
    "\n",
    "    \"\"\"\n",
    "    cnt=0  \n",
    "    data=[] # create an empty list to collect the data\n",
    "    pub_keywords=[]\n",
    "    last_key=[]\n",
    "    \n",
    "    # open the file and read through it line by line\n",
    "    with open(filepath, 'r') as file_object:\n",
    "        line = file_object.readline()\n",
    "        while line:\n",
    "            # at each line check for a match with a regex\n",
    "            key, match = _parse_line(line)\n",
    "\n",
    "            if key == 'pub_PMID': #first line of a new publication is always PMID\n",
    "                if cnt>0:\n",
    "                    data.append({'PMID': pub_PMID,\\\n",
    "                                 'title':pub_title,\\\n",
    "                                 'keywords':pub_keywords,\\\n",
    "                                 'abstract':pub_abstract,\\\n",
    "                                 'affiliation': list(set(pub_affiliation))\\\n",
    "                                })\n",
    "\n",
    "                cnt+=1\n",
    "                pub_keywords=[]\n",
    "                pub_affiliation=[]\n",
    "                pub_abstract=[]\n",
    "                pub_title=[]\n",
    "                pub_PMID = match.group('pub_PMID')\n",
    "            elif key == 'pub_title':\n",
    "                pub_title = match.group('pub_title')             \n",
    "            elif key == 'pub_keywords':\n",
    "                pub_keywords.append(match.group('pub_keywords').lower())\n",
    "            elif key == 'pub_affiliation':\n",
    "                pub_affiliation.append(match.group('pub_affiliation'))\n",
    "            elif key == 'pub_abstract':\n",
    "                pub_abstract=match.group('pub_abstract')\n",
    "            elif (key == 'line_break'):\n",
    "                if last_key:\n",
    "                    nline=match.group('line_break')\n",
    "                    if last_key == 'pub_keywords':\n",
    "                        pub_keywords[-1]=pub_keywords[-1]+' '+nline.lower()\n",
    "                    if last_key == 'pub_affiliation':\n",
    "                        pub_affiliation[-1]=pub_affiliation[-1]+' '+nline\n",
    "                    elif last_key == 'pub_title':\n",
    "                        pub_title = pub_title + ' '+nline\n",
    "                    elif last_key == 'pub_abstract':\n",
    "                        pub_abstract = pub_abstract + ' '+nline\n",
    "              \n",
    "            if not(key == 'line_break'):\n",
    "                last_key=key\n",
    "            line = file_object.readline()\n",
    "\n",
    "        # create a pandas DataFrame from the list of dicts\n",
    "        \n",
    "        # set the School, Grade, and Student number as the index\n",
    "#         data.set_index(['TI'], inplace=True)\n",
    "        # consolidate df to remove nans\n",
    "#         data = data.groupby(level=data.index.names).first()\n",
    "        # upgrade Score from float to integer\n",
    "#         data = data.apply(pd.to_numeric, errors='ignore')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = parse_file(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in data[0].keys():\n",
    "    print(\"{}:\\t {}\".format(k,data[0][k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uk=pd.Series([item for d in data[0:500] for item in d['keywords']]).value_counts().index\n",
    "# print(\"%i unique keywords\" % (len(uk)))\n",
    "# data=data[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath[:-3]+'json'\n",
    "import json\n",
    "with open(filepath[:-3]+'json', 'w') as fp:\n",
    "    json.dump(data, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "def clean_keyword_list(keywords):\n",
    "    keywords=[k.strip(\"*\") for k in keywords] #remove *\n",
    "    keywords=[k.replace(\"-\",\" \") for k in keywords] #remove -\n",
    "    keywords=[k for k in keywords if not(k.lower()==\"none\")] #remove \"none\"\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    keywords_temp=[]\n",
    "    for k in keywords: \n",
    "        keywords_temp.append( ' '.join([lemmatizer.lemmatize(w) for w in nltk.word_tokenize(k)])) #to singular\n",
    "    keywords=keywords_temp\n",
    "    keywords=[k if not(k==\"adc\") else \"antibody drug conjugate\" for k in keywords] #adc=antibody drug conjugate\n",
    "    regex=re.compile('antibody drug conjugate (.*)')\n",
    "    keywords=[k if not(re.match(regex,k)) else \"antibody drug conjugate\" for k in keywords] #remove \"antibody drug conjugate ( adc )\"\n",
    "    regex=re.compile('adc(.*)antibody drug conjugate(.*)')\n",
    "    keywords=[k if not(re.match(regex,k)) else \"antibody drug conjugate\" for k in keywords] #remove \"antibody drug conjugate ( adc )\"\n",
    "    regex=re.compile('adc(.*)antibody drug conjugate(.*)')\n",
    "    keywords=[re.sub(r\"[^\\w]\", \" \", k) for k in keywords] #remove \"periods\"\n",
    "    keywords=[re.sub(' +', ' ', k) for k in keywords] #remove multiple spaces\n",
    "    keywords=[re.sub(' +', ' ', k).strip() for k in keywords] #remove multiple spaces and the trailing space\n",
    "    keywords=[k if not(k==\"adc\") else \"antibody drug conjugate\" for k in keywords] #adc=antibody drug conjugate\n",
    "    keywords=list(set(keywords)) #remove duplicates\n",
    "    return keywords\n",
    "\n",
    "def group_similar_keywords(keyword_list):\n",
    "    df_keywords=pd.DataFrame(data=keyword_list,columns=['keyword'])\n",
    "    df_keywords['counts']=1\n",
    "    df_keywords=df_keywords.groupby('keyword').count().sort_values(by='counts', ascending=False).reset_index()\n",
    "    print(df_keywords.head(10))\n",
    "\n",
    "    t0=time.time()\n",
    "    print('calculating Levenshtein distances...')\n",
    "    keyword_list_unique=df_keywords['keyword'].values\n",
    "    dst=pdist(np.array(keyword_list_unique).reshape(-1,1), lambda x,y: 1-lev.ratio(x[0],y[0]))\n",
    "    print('done, time elapsed: %i s' % (time.time()-t0))\n",
    "\n",
    "    Z=sch.linkage(dst,'ward')\n",
    "    cluster_assignments=sch.fcluster(Z,0.1, criterion='distance')\n",
    "    freq=np.bincount(cluster_assignments)\n",
    "    cluster_ids=np.arange(len(freq))[freq>1]\n",
    "\n",
    "    df_keywordcluster=pd.DataFrame(data=[],columns=['grouped_keywords','master_keyword'])\n",
    "    for cid in cluster_ids:\n",
    "        temp_keywordlist=keyword_list_unique[[np.arange(len(keyword_list_unique))[cluster_assignments==cid]]]\n",
    "        temp_counts=[df_keywords.loc[lambda x: x.keyword==kw,'counts'].values[0] for kw in temp_keywordlist]\n",
    "        temp_winner_keyword=temp_keywordlist[np.array(temp_counts).argmin()]\n",
    "        df_keywordcluster=df_keywordcluster.append(pd.DataFrame([[temp_keywordlist,\\\n",
    "                                                                  temp_winner_keyword]],\\\n",
    "                                                                columns=df_keywordcluster.columns), ignore_index=True)\n",
    "\n",
    "    print(df_keywordcluster.to_string())#  \n",
    "    df_keywordcluster=pd.DataFrame(df_keywordcluster['grouped_keywords'].to_list(), index=df_keywordcluster.master_keyword).stack().reset_index(name='old_keyword')\n",
    "    df_keywordcluster=df_keywordcluster.drop(columns=['level_1'])\n",
    "    \n",
    "    return df_keywordcluster\n",
    "\n",
    "def replace_similar_keywords(keywords, df_keyword_simplification):\n",
    "    for cnt,k in enumerate(keywords):\n",
    "        if df_keyword_simplification['old_keyword'].isin([k]).sum():\n",
    "             keywords[cnt]=df_keyword_simplification['master_keyword'][df_keyword_simplification['old_keyword'].isin([k])].values[0]\n",
    "#             print(df_keyword_simplification['master_keyword'][df_keyword_simplification['old_keyword'].isin([k])][0],df_keyword_simplification['old_keyword'][df_keyword_simplification['old_keyword'].isin([k])][0])\n",
    "#             print(df_keyword_simplification['master_keyword'][df_keyword_simplification['old_keyword'].isin([k])].values[0],df_keyword_simplification['old_keyword'][df_keyword_simplification['old_keyword'].isin([k])].values[0],'\\n')\n",
    "#         keywords[cnt]=k\n",
    "    return keywords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('initially %d keywords' % len(np.unique([kw for d in data for kw in d['keywords']])))\n",
    "\n",
    "for cnt,d in enumerate(data):\n",
    "    for key in d:\n",
    "        if key == \"keywords\":\n",
    "            data[cnt][key]=clean_keyword_list(d[key])\n",
    "            \n",
    "print('after cleaning %d keywords' % len(np.unique([kw for d in data for kw in d['keywords']])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_list=[kw for d in data for kw in d['keywords']]\n",
    "df_keyword_simplification=group_similar_keywords(keyword_list)\n",
    "\n",
    "print('initially %d keywords' % len(np.unique([kw for d in data for kw in d['keywords']])))\n",
    "\n",
    "for cnt,d in enumerate(data):\n",
    "    for key in d:\n",
    "        if key == \"keywords\":\n",
    "            data[cnt][key]=replace_similar_keywords(d[key],df_keyword_simplification)\n",
    "            \n",
    "print('after keyword similarity check: %d keywords' % len(np.unique([kw for d in data for kw in d['keywords']])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as sch\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "# dendrogram = sch.dendrogram(sch.linkage(points, method='ward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cnt in range(len(lev_ratio)-1):\n",
    "     if lev.ratio(keyword_list[cnt],keyword_list[cnt+1])>0.9:\n",
    "            print([cnt, keyword_list[cnt], keyword_list[cnt+1], lev.ratio(keyword_list[cnt],keyword_list[cnt+1]), lev.distance(keyword_list[cnt],keyword_list[cnt+1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data=keyword_list, columns=['kw'])['kw'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keywords['keyword'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sch.ward(lev_ratio)\n",
    "t0=time.time()\n",
    "d=pdist(np.array(keyword_list).reshape(-1,1), lambda x,y: 1-lev.ratio(x[0],y[0]))\n",
    "print(time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(squareform(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(keyword_list[56],keyword_list[89])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z=sch.linkage(d,'ward')\n",
    "# np.array(keyword_list)[sch.fcluster(Z,0.3, criterion='distance')]\n",
    "a=sch.fcluster(Z,0.1, criterion='distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq=np.bincount(a)\n",
    "cluster_ids=np.arange(len(freq))[freq>1]\n",
    "for cid in cluster_ids:\n",
    "    print(np.array(keyword_list)[[np.arange(len(keyword_list))[a==cid]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(keyword_list)[[1619,1988]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.cluster\n",
    "import distance\n",
    "\n",
    "words = \"YOUR WORDS HERE\".split(\" \") #Replace this line\n",
    "words = np.asarray(words) #So that indexing with a list will work\n",
    "lev_similarity = -1*np.array([[distance.levenshtein(w1,w2) for w1 in words] for w2 in words])\n",
    "\n",
    "affprop = sklearn.cluster.AffinityPropagation(affinity=\"precomputed\", damping=0.5)\n",
    "affprop.fit(lev_similarity)\n",
    "for cluster_id in np.unique(affprop.labels_):\n",
    "    exemplar = words[affprop.cluster_centers_indices_[cluster_id]]\n",
    "    cluster = np.unique(words[np.nonzero(affprop.labels_==cluster_id)])\n",
    "    cluster_str = \", \".join(cluster)\n",
    "    print(\" - *%s:* %s\" % (exemplar, cluster_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "dn=sch.dendrogram(Z)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lev.ratio('adc','antibody drug conjugate')\n",
    "# words=np.asarray(a)\n",
    "# for cluster_id in np.unique(affprop.labels_):\n",
    "#     exemplar = words[affprop.cluster_centers_indices_[cluster_id]]\n",
    "#     cluster = np.unique(words[np.nonzero(affprop.labels_==cluster_id)])\n",
    "#     cluster_str = \", \".join(cluster)\n",
    "#     print(\" - *%s:* %s\\n\" % (exemplar, cluster_str))\n",
    "regex=re.compile('(.*) antibody drug conjugate')\n",
    "regex=re.compile('adc(.*)antibody drug conjugate(.*)')\n",
    "for item in keyword_list:\n",
    "    if (re.match(regex,item)):\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(lev_ratio[0])):\n",
    "for cnt1 in range(len(keyword_list)):\n",
    "    for cnt2 in range(len(keyword_list)):\n",
    "        if (1-lev_ratio[cnt1][cnt2]<0.1) and (1-lev_ratio[cnt1][cnt2]>0):\n",
    "            print([keyword_list[cnt1],keyword_list[cnt2],1-lev_ratio[cnt1][cnt2]])\n",
    "        if cnt2>20000:\n",
    "            break\n",
    "    if cnt1>500:\n",
    "        break\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build list of keywords that appear together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_list_filepath=filepath[0:-4]+\"_keywordlinks.csv\"\n",
    "link_list_counted_filepath=filepath[0:-4]+\"_keywordlinks_counted.csv\"\n",
    "count=len(data)\n",
    "link_list=[]\n",
    "cnt=0\n",
    "for cnt, pubdata in enumerate(data): #loop through all publications and establish pairwise links of keywords\n",
    "    keywords=pubdata['keywords']\n",
    "    if not[]:\n",
    "        keywords=clean_keyword_list(keywords)\n",
    "        for i in np.arange(0,len(keywords)-1):\n",
    "            for k in np.arange(i+1,len(keywords)):\n",
    "                keywordpair_temp=[keywords[i].lower(),keywords[k].lower()]  #alphabetical order to ensure that pairs A/B\\\n",
    "                                                                            #do not show as B/A\n",
    "                                                                            #lower case to avoid issues with inconsistent\n",
    "                                                                            #capitalization\n",
    "                keywordpair_temp.sort()\n",
    "                keywordpair_temp.append(pubdata['PMID']) #append PMID to avoid double-countings at a later stage\n",
    "                link_list.append(keywordpair_temp)\n",
    "                \n",
    "    if not(cnt%1000):\n",
    "        print(\"pairwise linking of keywords: %i of %i done\" % (cnt, count))\n",
    "        \n",
    "        \n",
    "link_list_df=pd.DataFrame(data=link_list,columns=[\"keyword1\",\"keyword2\",\"PMID\"])\n",
    "link_list_df.to_csv(link_list_filepath)\n",
    "link_list_df=link_list_df.drop_duplicates()\n",
    "link_list_df=link_list_df.groupby([\"keyword1\",\"keyword2\"]).count().reset_index() #count how often each pair appears\n",
    "link_list_df=link_list_df.rename(columns={\"PMID\":\"counts\"})\n",
    "\n",
    "print(\"established %i keyword links\" % (len(link_list)))   \n",
    "print(\"established %i unique keyword links\" % (len(link_list_df)))\n",
    "link_list_df.to_csv(link_list_counted_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniquekeyword_series=pd.Series([item for d in data for item in d['keywords']])\n",
    "uk=uniquekeyword_series.value_counts()\n",
    "maxcat=20\n",
    "f=plt.figure(figsize=(16, 11))\n",
    "ax=sns.barplot(x=uk.index[0:maxcat],y=uk.to_list()[0:maxcat])\n",
    "for item in ax.get_xticklabels():\n",
    "    item.set_rotation(90)\n",
    "ax.set_title('Top20 keywords'+\"\\n\"+filepath[0:-4])\n",
    "ax.set_ylabel('counts')\n",
    "f.savefig(filepath[0:-4]+\"_topkeywords.png\",bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_df=link_list_df.copy()\n",
    "plt_df['label']=\"[\"+plt_df['keyword1']+\"] - [\"+plt_df['keyword2'] + \"]\"\n",
    "plt_df=plt_df.sort_values(by=['counts'], ascending=False)\n",
    "plt_df['label']=\"[\"+plt_df['keyword1']+\"] - [\"+plt_df['keyword2'] + \"]\"\n",
    "maxcat=20\n",
    "\n",
    "f=plt.figure(figsize=(16, 11))\n",
    "ax=sns.barplot(x=plt_df.iloc[0:maxcat]['label'],y=plt_df.iloc[0:maxcat]['counts'])\n",
    "for item in ax.get_xticklabels():\n",
    "    item.set_rotation(90)\n",
    "ax.set_title('Top20 keyword pairs'+\"\\n\"+filepath[0:-4])\n",
    "ax.set_ylabel('counts')\n",
    "f.savefig(filepath[0:-4]+\"_topkeywordpairs.png\",bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]['affiliation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.pos_tag(['German'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_lg\n",
    "nlp=spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not(not([]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in range(50):\n",
    "    print(ind)\n",
    "    doc=data[ind]['affiliation'][0].split(',')\n",
    "    doc=[item.strip() for item in doc]\n",
    "    doc_temp=[]\n",
    "    for cnt, d in enumerate(doc):\n",
    "        if (cnt>0) & (d in [\"inc.\", \"inc\", \"corp\", \"corp.\"]):\n",
    "            doc_temp[cnt-1]=doc_temp[cnt-1]+\" \"+d\n",
    "        else:\n",
    "            doc_temp.append(d)\n",
    "    \n",
    "    doc = [nlp(item) for item in doc_temp]\n",
    "    for d in doc:\n",
    "        print([(X, X.text, X.label_) for X in d.ents])\n",
    "    \n",
    "    doc = [nlp(item) for item in data[ind]['affiliation'][0]]\n",
    "    for d in doc:\n",
    "        print([(X, X.text, X.label_) for X in d.ents])\n",
    "        \n",
    "    print(data[ind]['affiliation'][0].split(','))\n",
    "    print(doc_temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in range(50):\n",
    "    doc=data[ind]['affiliation'][0].split(',')\n",
    "    print(doc[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
